1. TERRAFORM set up for SQL 

- Mise en place des variables d environnement dans le .env 
Chargement des variables dans votre environnement actuel : ```source .env```
- terraform init
- terraform plan
- terraform apply (-auto-approve)

2. PREMIER EXPORT DES VIDEOS DE 
L ADCC
	- https://www.youtube.com/watch?v=UolTykPneHc&t=11s -> UolTykPneHc
	- https://www.youtube.com/watch?v=wEn6ugcbGV8&t=360s -> wEn6ugcbGV8
FLOWGRAPPLING
	- https://www.youtube.com/watch?v=0smqvO2iQrE
	- https://www.youtube.com/watch?v=vOKZpPQFVgY

3. Installez Apache Airflow dans un nouvel environnement virtuel :
(Ne pas oublier d'y joindre les differentes variables d'env necessaires au bon fonctionnement du projet : GCP-key.json ...)
Airflow nécessite une base de données pour stocker son état et ses configurations. Pour une installation locale simple, vous pouvez utiliser SQLite -> de nombreux problemes sont apparus, notamment sur l execution simultanee lors du trigger d event, changement de database avec mise en place d un client SQL.
- pip install apache-airflow

- airflow db init

- airflow users create \
    --username victordeleusse \
    --firstname victor \
    --lastname deleusse \
    --role Admin \
    --email victordeleusse@gmail.com

Password: 1234
 
Démarrez le serveur web d'Airflow (accessible par défaut sur http://localhost:8080) :
- airflow webserver --port 8080

Check si l environnement d Airflow est bien configure en demarrant le scheduler
- airflow scheduler

# Exécution Manuelle : Pour un test immédiat, vous pouvez également déclencher manuellement l'exécution de votre DAG via l'interface utilisateur web d'Airflow ou en utilisant la commande CLI airflow dags trigger update_youtube_comments.



BigQuery est un entrepôt de données entièrement géré conçu pour le traitement et l'analyse de big data avec une syntaxe SQL.